---
title: "Loire River Graduate Student Descent"
author: "Matthew Ross"
date: "3/9/2022"
output:
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r, include = F}

library(tidyverse) #y'all know it
library(xgboost)
library(lubridate) #datetime goodness
library(ggthemes) #get rid of gray grid
library(scales) #transform x axes to look nicer
```

# Intro

A friend told me today about a term called ["Grad student descent"](https://sciencedryad.wordpress.com/2014/01/25/grad-student-descent/) which is a 
play on words with the term gradient descent, an extremely common tool used to 
tune hyperparameters (and parameters) for machine learning models. Grad student
descent then is when that method is simply having grad students search a parameter
space manually in part to improve the model, and in part to learn how these
algorithms work.

Today we are trying to predict the concentrations of either chlorophyll a (`CHLA`) 
or total suspended sediment (`TSS`) in the Loire River France using satellite imagery.
The grab sample data (someone grabbing water from a stream for analysis), comes
from the French electricity agency who collects this data to meet their water
quality standards. 
Basically the core idea here is that water color as captured in an image tells you
something about what is in the water. If it's green and bright (lots of light 
reflected from the water), then it may have a lot of algae (Chlorophyll a is a proxy
for algal biomass). If it's tan and bright, it may have lots of sediment. If it's
dark and blue it's clear!

This color information is captured in "bands" like red, blue, green etc... from
the Landsat series of satellites, which have collected data over the world since
the late 70s. For this analysis we are only using Landsat 5,7, and 8, so the
data goes back to 1984. 


## Data read

```{r}
wq_sr <- read_csv('C:/Users/13074/Documents/ESS580/Loire_ML/data/wq_sr.csv') %>%
  mutate(year = year(date))

```


## Model development


This is the fiddly bit. The goal? Tune hyperparameters to get the lowest
RMSE and the best Measured/Predicted fit (points close to the red line). 


```{r}
# A vector that contains all the columns I want to keep. 
predictors <- c("azimuth", "blue","green", "nir" ,"red", "swir1","swir2","zenith"  , "NR" ,"BR" ,"GR" , "SR" ,    "BG" , "BN", "BS" ,"GS" ,"GN" , "ndvi", "ndwi", 'fai',  "hillshade", "hue", "bright_tot", "bright")

booster <- function(df = wq_sr,pred='CHLA',title='Chlorophyll a',
                    features = predictors){
  
  non_nas <- (!is.na(df[,pred]))
  
  #remove nas
  df = df[non_nas,]
  
  
  #Sample 60% of the data. 
  train <- df %>%
    sample_frac(0.6) 
  
  #Keep only data that is not in train
  #How could we make this safer (spatiotemporal robustness)
  test <- df %>%
    anti_join(.,train,by='index')
  
  
  # Actual boosting model
  ag_mod <- xgboost(data=train %>%
                    dplyr::select(features) %>% 
                      as.matrix(.),
                  label = train %>% pull(pred) %>% log10(.),
                  nthread=3, #4, didn't change much.
                  max_depth=9, #4 lowered it.
                  eta=0.3, #0.2 lowered it
                  gamma = 0.1, #0.1 Everything went up! leaving alone
                  nrounds=300, #300
                  subsample = 0.5, #best_par['subsample'], # 0.5, moving this up or down  and it went up
                  colsample_bytree = 0.5,  #0.5, Nope.
                  min_child_weight = 1, #1 
                  lambda =3, #3
                  print_every_n = 100)
  
  #Tune ntree, k, numcut, bands that you use, etc...
  
  
  #apply predictions. 
  test <- test %>% 
    mutate(bpred= 10^predict(ag_mod,test %>%
                               dplyr::select(features) %>% 
                      as.matrix(.)))
  #Optional for log
  #test[,pred] = 10^test[,pred]
  
  #Remove NAs
  test <- test %>%
    filter(!is.na(pred)) %>%
    as.data.frame()
  
  error <- 
    tibble(rmse=Metrics::rmse(test$bpred,test[,pred]),
                       mdae=Metrics::mdae(test$bpred,test[,pred]),
                       mape=Metrics::mape(test$bpred,test[,pred]),
                       bias=Metrics::bias(test$bpred,test[,pred]))


g1 <- ggplot(test,
         aes_string(x=pred,y='bpred',color='year'))  +
    geom_point() + 
    geom_abline(intercept=0,slope=1,color='red') +
    labs(x='Measured',y='Predicted') + 
    theme_few() +
  scale_color_viridis_c() + 
     scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x)))  +
       scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x))) + 
  ggtitle(title)


# returns a list of the plot (g1), the error metric (error), and the 
# model (ag_mod)
return(list(g1,error,ag_mod))
}





```



## Chl-a model

```{r}

set.seed(2020526) ## Makes the work reproducible, but you can hack this!2020526
chl_boost <- booster(df = wq_sr)


```


### Chl-a Model Measured/Predicted Plot

```{r}
chl_boost[[1]]
```


### Chl-a Error Metrics


```{r}
chl_boost[[2]]
```


## TSS Model

```{r}
# A vector that contains all the columns I want to keep. 
predictors <- c("azimuth", "blue","green", "nir" ,"red", "swir1","swir2","zenith"  , "NR" ,"BR" ,"GR" , "SR" ,    "BG" , "BN", "BS" ,"GS" ,"GN" , "ndvi", "ndwi", 'fai',  "hillshade", "hue", "bright_tot", "bright")

booster2 <- function(df = wq_sr,pred='TSS',title='Total Suspended Solids',
                    features = predictors){
  
  non_nas <- (!is.na(df[,pred]))
  
  #remove nas
  df = df[non_nas,]
  
  
  #Sample 60% of the data. 
  train <- df %>%
    sample_frac(0.6) 
  
  #Keep only data that is not in train
  #How could we make this safer (spatiotemporal robustness)
  test <- df %>%
    anti_join(.,train,by='index')
  
  
  # Actual boosting model
  ag_mod <- xgboost(data=train %>%
                    dplyr::select(features) %>% 
                      as.matrix(.),
                  label = train %>% pull(pred) %>% log10(.),
                  nthread=4, #4, Increased with any change
                  max_depth=4, #4 Increased with any change
                  eta=0.2, #0.2 Increased with any change 
                  gamma = 0.08, #0.1 decreased
                  nrounds=350, #300 decreased
                  subsample = 0.5, #best_par['subsample'], # 0.5 Increased with any change
                  colsample_bytree = 0.5,  #0.5, Increased with any change
                  min_child_weight = 1, #1  Increased with any change
                  lambda =3, #3 Increased with any change
                  print_every_n = 100)
  
  #Tune ntree, k, numcut, bands that you use, etc...
  
  
  #apply predictions. 
  test <- test %>% 
    mutate(bpred= 10^predict(ag_mod,test %>%
                               dplyr::select(features) %>% 
                      as.matrix(.)))
  #Optional for log
  #test[,pred] = 10^test[,pred]
  
  #Remove NAs
  test <- test %>%
    filter(!is.na(pred)) %>%
    as.data.frame()
  
  error <- 
    tibble(rmse=Metrics::rmse(test$bpred,test[,pred]),
                       mdae=Metrics::mdae(test$bpred,test[,pred]),
                       mape=Metrics::mape(test$bpred,test[,pred]),
                       bias=Metrics::bias(test$bpred,test[,pred]))


g2 <- ggplot(test,
         aes_string(x=pred,y='bpred',color='year'))  +
    geom_point() + 
    geom_abline(intercept=0,slope=1,color='red') +
    labs(x='Measured',y='Predicted') + 
    theme_few() +
  scale_color_viridis_c() + 
     scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x)))  +
       scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x))) + 
  ggtitle(title)


# returns a list of the plot (g1), the error metric (error), and the 
# model (ag_mod)
return(list(g2,error,ag_mod))
}





```



### TSS plot


```{r}

set.seed(2020526) ## Makes the work reproducible, but you can hack this!2020526
tss_boost <- booster2(df = wq_sr)

```


### TSS Evaluation

```{r}
tss_boost[[1]]

tss_boost[[2]]


```



# Q1) 
What is the best CHLA RMSE you could get after 50 minutes of fiddling? 

*After more than 50 minutes of fiddling with the hyperparameters, I was only able to get a rmse of 24.0 by lowering the nthread to 3, increasing the max_depth to 9 and increasing the eta to 0.3. Altering any other hyperparameters or the seed number would cause the rmse to increase. See #notes in code chunk for further details.*

# Q2) 
What is the best TSS RMSE you could get after 50 minutes of fiddling?

*Using the hyperparameters originally set by the Chlorophyll A booster function, the rmse is 60.5. I was able to lower the rmse to 50.8 by decreasing the gamma to 0.08, as well as increasing the nrounds to 350. Altering any other hyperparameters or the seed number caused the rmse to increase.* 

